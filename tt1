Here is the data flow for your **Classic RAG (Vector-First)** architecture, explicitly mapped to your defined on-premise technology stack (**OpenTrek, pgvector, Redis, Isilon**).

This flow illustrates the "Happy Path" where a user asks a question, and the system retrieves answers using vector similarity.

### The Architecture Diagram

```mermaid
graph TD
    %% Users and Entry
    User([Bank Employee]) -->|1. Query| Gateway[Alibaba OpenTrek\n(Orchestration & Security Layer)]

    %% Caching Layer
    Gateway -->|2. Check Cache| Redis[(Redis\nCache)]
    Redis -.->|Hit: Return Cached Answer| Gateway
    Redis -.->|Miss: Proceed| Gateway

    %% Vectorization (Inference)
    subgraph "Inference & Embedding"
        Gateway -->|3. Send Query| EmbedModel[Embedding Model\n(Frozen)]
        EmbedModel -->|4. Return Vector| Gateway
    end

    %% Retrieval Layer
    subgraph "Knowledge Retrieval"
        Gateway -->|5. Similarity Search\n(Query Vector)| PgVector[(pgvector\nPostgreSQL)]
        PgVector -->|6. Return Top-K Chunks\n+ Metadata| Gateway
        
        %% Optional: Fetch full text if not in vector DB
        Gateway -.->|7. Fetch Raw Content\n(If needed)| Isilon[(Dell Isilon\nSource Docs)]
    end

    %% Generation Layer
    subgraph "Generation (LLM)"
        Gateway -->|8. Construct Prompt\n(System Prompt + Context + Query)| LLM[LLM Inference\n(Qwen / Frozen)]
        LLM -->|9. Generate Response| Gateway
    end

    %% Final Output
    Gateway -->|10. Final Answer| User
```

-----

### Step-by-Step Data Flow Description

#### Phase 1: Request & Caching (The "Speed" Layer)

  * **Step 1 (Input):** The user submits a query (e.g., *"What is the counterparty risk policy for Class A derivatives?"*) to the **Alibaba OpenTrek** API Gateway.
      * *Security Note:* This is where input sanitization (PII redaction/Injection checks) happens.
  * **Step 2 (Cache Check):** OpenTrek hashes the query and checks **Redis**.
      * **Hit:** If this exact question was asked recently, **Redis** returns the stored answer immediately (saving GPU costs).
      * **Miss:** The system proceeds to the retrieval pipeline.

#### Phase 2: Vector Retrieval (The "Search" Layer)

  * **Step 3 (Vectorization):** OpenTrek sends the raw text query to the **Embedding Model** (hosted within the OpenTrek runtime).
  * **Step 4 (Embedding):** The model converts the text into a numerical vector (e.g., a 1536-dimensional float array).
  * **Step 5 (Search):** OpenTrek sends this vector to **pgvector** (PostgreSQL). It performs an Approximate Nearest Neighbor (ANN) search (e.g., using HNSW or IVFFlat indexes) to find the most similar document chunks.
  * **Step 6 (Retrieval):** **pgvector** returns the "Top-K" chunks (e.g., the 5 most relevant paragraphs) along with their metadata (page number, document source).
      * *Note on Elasticsearch:* While your stack includes Elasticsearch, a *classic vector RAG* relies primarily on `pgvector` here. Elasticsearch would typically be used only if you needed keyword-specific filtering (e.g., "Find vectors ONLY in documents dated 2024").

#### Phase 3: Generation (The "Intelligence" Layer)

  * **Step 8 (Prompt Assembly):** OpenTrek acts as the "Chef." It combines three ingredients into a single text block:
    1.  **System Prompt:** *"You are a helpful banking assistant..."* (The immutable safety rules).
    2.  **Context:** The text chunks retrieved from **pgvector**.
    3.  **User Query:** The original question.
  * **Step 9 (Inference):** This massive block of text is sent to the **LLM (e.g., Qwen)**. The LLM reads the retrieved context and generates an answer rooted in that data.
  * **Step 10 (Response):** The answer is streamed back to the user via OpenTrek.
      * *Post-Processing:* The answer is simultaneously saved to **Redis** to speed up future requests.

### Key Component Roles Summary

| Component | Role in RAG Flow |
| :--- | :--- |
| **Alibaba OpenTrek** | **The Brain.** Orchestrates the flow, calls models, manages the prompt construction, and enforces security. |
| **Redis** | **The Accelerator.** Stores "Question -\> Answer" pairs to reduce latency. |
| **pgvector** | **The Index.** Stores the mathematical representation of your bank's knowledge. Performs the actual "search." |
| **Dell Isilon** | **The Library.** Stores the original "Gold Copy" PDF/Docx files. If `pgvector` only stores vectors, OpenTrek fetches the actual readable text from here (or a text cache) to show the user. |



In this architecture, **Model File Hosting** is not a static concept; it is a **tiered lifecycle** moving from "Cold Storage" (Governance) to "Hot Memory" (Inference).

Because LLMs like Qwen-72B are massive (40GB–140GB+), you cannot stream them over the network for every request. The architecture relies on a **Fetch-Cache-Load** pipeline to balance security (on Isilon) with performance (on GPU).

### 1. The Storage Hierarchy

The system uses a three-tier storage strategy to bridge the gap between your massive storage (Isilon) and your high-speed compute (GPUs).

| Tier | Component | Function | State | Speed |
| :--- | :--- | :--- | :--- | :--- |
| **1. The Vault** | **Dell Isilon** | **Long-term Storage.** Holds the "Golden Copy" of signed `.safetensors` files. | **Cold** (Encrypted At Rest) | Slow (Network Speed) |
| **2. The Cache** | **Compute Node NVMe** | **Local Staging.** The ephemeral SSD on the OpenTrek server where models are copied prior to loading. | **Warm** (Ephemeral) | Fast (Disk Speed) |
| **3. The Engine** | **GPU VRAM** | **Runtime.** The High Bandwidth Memory (HBM) on the Nvidia GPUs where the model "lives" while serving traffic. | **Hot** (Active) | Ultra-Fast |

---

### 2. The Data Flow: From Vault to VRAM

This diagram illustrates the precise sequence of events when Alibaba OpenTrek initializes a model service.



#### **Phase A: The Governance Handshake (PostgreSQL)**
Before any file moves, OpenTrek consults the **Model Registry** in **PostgreSQL**.
1.  **Request:** OpenTrek Orchestrator asks: *"I need to deploy Qwen-72B-Chat-Int4."*
2.  **Validation:** Postgres checks:
    * Is the status `APPROVED`?
    * Does the deployment environment match the allowed scope?
    * What is the expected SHA-256 hash?
3.  **Result:** Postgres returns the filepath on Isilon (e.g., `/isilon/ai-models/qwen/v1.2/model.safetensors`).

#### **Phase B: The Secure Fetch (Isilon → Node)**
OpenTrek does **not** mount Isilon directly to the container (to avoid file locking issues).
1.  **Copy:** The Orchestrator copies the model file from **Dell Isilon** to the **Local NVMe SSD** of the specific inference node.
2.  **Verification (The Gatekeeper):** Once copied, the node calculates the hash of the local file.
    * *Critical Check:* `If Local_Hash != Postgres_Hash -> DELETE FILE & ABORT.`
3.  **Decryption:** If files are encrypted at rest on Isilon, they are decrypted into the local volume (or into memory) at this stage using the KMS key.

#### **Phase C: The Runtime Load (Node → GPU)**
1.  **Memory Mapping (`mmap`):** The inference engine (e.g., vLLM or TRT-LLM running inside OpenTrek) uses `mmap` to map the `.safetensors` file into the system RAM.
2.  **Tensor Sharding:** If using multiple GPUs, the model is split (tensor parallelism) and loaded into the **VRAM** of the GPUs.
3.  **Locking:** The model is now "Hot." The local file on the NVMe disk is locked or treated as a read-only cache.

---

### 3. Security Controls in the Hosting Flow

To align with your NIST/OWASP requirements, security is injected at specific transfer points:

* **During Phase A (Registry):**
    * **Role-Based Access Control (RBAC):** Only the OpenTrek Service Account (not human users) has permission to read the model paths from Postgres.
* **During Phase B (Fetch):**
    * **Network Segmentation:** The traffic from Isilon to the Compute Node flows over a dedicated **Storage VLAN**, separate from the User API traffic.
    * **Ephemeral Existence:** The model copy on the Local NVMe is deleted immediately when the container shuts down (no sensitive IP left on disk).
* **During Phase C (Load):**
    * **Admission Control:** The cryptographic signature (signed by your internal PKI) is verified one last time by the runtime before `cudaMemcpy` moves data to the GPU.

### 4. Why this approach?
* **Performance:** You only pay the "Network Tax" (copying from Isilon) once—when the pod starts. Subsequent queries hit the GPU VRAM (milliseconds).
* **Stability:** If Isilon goes down briefly, the running models keep working because they are already loaded in VRAM/Local Disk.
* **Safety:** The "Golden Copy" on Isilon is never directly exposed to the inference engine's potential exploits.



In the context of your AI Factory, **Vector Embeddings** are the "currency" of the system. They represent your banking data converted into mathematical coordinates.

Unlike the *Model Weights* (which are static binaries), **Embeddings are dynamic data assets** generated continuously as new documents are ingested. Their lifecycle requires a distinct flow for generation, storage, and protection.

### 1. The Embedding Lifecycle Data Flow

This diagram illustrates how a raw policy document becomes a searchable vector in your architecture.



#### **Phase A: The "Embedding Factory" (Generation)**
This occurs within **Alibaba OpenTrek**.
1.  **Chunking (CPU):** A PDF from **Dell Isilon** is broken into 512-token "chunks" (paragraphs).
2.  **Tokenization:** Each chunk is converted into integer tokens using the specific tokenizer of your embedding model (e.g., `bert-base-uncased` or `text-embedding-ada-002` equivalent).
3.  **Inference (GPU):** The tokens are passed to the **Embedding Model** loaded in GPU memory.
    * *Note on Hosting:* The Embedding Model itself follows the same "Fetch-Load" lifecycle as the LLM (Isilon -> NVMe -> GPU), but it is usually much smaller (e.g., 500MB vs 40GB) and handles higher throughput.
4.  **Output:** The model outputs a `float32` array (e.g., `[0.012, -0.931, ... 1536 dimensions]`).

#### **Phase B: The "Vector Vault" (Storage)**
Vectors are not stored in files; they are stored in the database.
1.  **Indexing (pgvector):** The vector array is sent to the **pgvector** database.
    * **HNSW Index:** To make search fast, pgvector builds a Hierarchical Navigable Small World (HNSW) graph. This organizes vectors so "similar" banking concepts sit "close" to each other in memory.
2.  **Metadata Linking:** The vector is stored alongside a Foreign Key pointing back to the original document ID in **PostgreSQL** (the Registry).
    * *Crucial:* The vector table usually looks like: `(id, embedding_vector, chunk_text, document_id, access_control_group)`.

---

### 2. Storage Strategy: Where do they live?

| Component | Content | Purpose | Security State |
| :--- | :--- | :--- | :--- |
| **Active Index** | **pgvector (RAM)** | Holds the HNSW graph for active searching. | **Hot**. If this server reboots, it rebuilds from disk. |
| **Persistence** | **pgvector (Disk)** | The Write-Ahead Log (WAL) and data files. Ensures vectors aren't lost on restart. | **Warm**. Encrypted at the volume level. |
| **Backup** | **Dell Isilon** | Periodic `pg_dump` snapshots of the vector table. | **Cold**. WORM-protected backups. |
| **Cache** | **Redis** | Stores vectors for *frequently accessed* documents (e.g., the "Holiday Policy"). | **Ephemeral**. TTL (Time To Live) of 24 hours. |

---

### 3. Specific Security Risks for Embeddings

Since you are in a G-SIB (Global Systemically Important Bank), you must treat embeddings as **Sensitive Data**. They are not just numbers; they are semantic representations of secrets.

#### **Risk 1: Model Inversion (Reconstructing Text)**
* **The Threat:** If an attacker steals your `pgvector` database, can they turn the vector `[0.12, ...]` back into the sentence "The CEO's bonus is $5M"?
* **The Reality:** Yes, research (e.g., "Vec2Text") shows that with enough vectors, you can approximate the original text.
* **The Control:**
    * **Database Encryption:** Treat the `pgvector` volume as High Confidentiality.
    * **Access Separation:** The DB user searching vectors (Inference Role) should **not** have permissions to "SELECT *" (bulk export) the entire table.

#### **Risk 2: Vector Poisoning (Search Bias)**
* **The Threat:** An attacker injects "poisoned" vectors that force the search engine to retrieve the wrong document for specific queries (e.g., mapping "Fraud Policy" queries to a "Picnic Policy" document).
* **The Control:**
    * **Signed Ingestion:** Only the trusted "Ingestion Pipeline" service account can INSERT into the `embeddings` table. No human access allowed.
    * **Drift Monitoring:** Monitor the mathematical "centroid" of your vector clusters. If the "Compliance" cluster suddenly shifts, it indicates data tampering.

### 4. Comparison: Embedding Model vs. LLM

To clarify the difference in hosting:

| Feature | Embedding Model (e.g., BGE-M3) | Generative LLM (e.g., Qwen-72B) |
| :--- | :--- | :--- |
| **Size** | Small (< 2GB) | Massive (> 40GB) |
| **Latency** | Milliseconds (Batch processing) | Seconds (Streaming generation) |
| **Throughput** | High (Can embed 1,000 docs/sec) | Low (Can answer 10 users/sec) |
| **Scaling** | Scale **Horizontally** (Add more small CPU/GPU nodes) | Scale **Vertically** (Need bigger GPUs/NVLink) |
| **Output** | Array of Numbers (Stored in DB) | Text (Sent to User) |

### Immediate Next Step

Since you are using **pgvector** in a high-compliance environment, a common bottleneck is **Row-Level Security (RLS)**. Standard RLS can slow down vector search significantly.

**Would you like me to outline the "Partitioning Strategy" for pgvector to maintain sub-second search speeds while strictly enforcing the bank's Access Control Lists (ACLs)?**
