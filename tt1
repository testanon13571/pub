### **Control Name:** Model Resource Consumption & Traffic Throttling (Rate Limiting)

In the world of AI, Rate Limiting is not just about preventing DoS attacks (Availability); it is a critical **Financial Control** (Cost Management) and a **Security Control** (Anti-Brute Force).

Because LLM inference is computationally expensive and billed by the "token," standard HTTP rate limiting (requests per second) is often insufficient. You must implement strategies that account for the **complexity** of the request, not just the count.

[Image of API Gateway rate limiting architecture diagram]

-----

### **1. The Core Differentiator: Human vs. Machine**

The control must apply different logic depending on the actor.

| Feature | **Human-to-Machine (H2M)** | **Machine-to-Machine (M2M)** |
| :--- | :--- | :--- |
| **Typical Actor** | Employee via Chatbot, Customer Support. | Automated Cron Job, API Integration, Slack Bot. |
| **Risk Profile** | Shadow IT, "Jailbreak" attempts, casual overuse. | Runaway loops, massive batch jobs, cost spikes. |
| **Control Unit** | **Sessions & User ID.** | **API Keys & Service Accounts.** |
| **Strategy** | "Soft" limits (queueing/throttling). | "Hard" limits (immediate 429 Error rejection). |

-----

### **2. Control Strategies & Implementation Levels**

You should layer these strategies. A simple "Requests Per Minute" cap is rarely enough for GenAI.

#### **Strategy A: Token-Aware Throttling (The "Cost" Metric)**

Standard rate limiters count *requests*. AI rate limiters must count *tokens*.

  * **The Problem:** User A sends 10 requests saying "Hello." User B sends 10 requests analyzing 50-page PDFs. User B costs you 1000x more, but a standard limiter treats them equally.
  * **The Control:** Implement **Token Bucket Rate Limiting**.
      * Assign each user/service a "Token Budget" (e.g., 50,000 tokens/minute).
      * Before processing, the gateway estimates the token count. If `Current_Usage + Estimated_Cost > Limit`, reject the request.

#### **Strategy B: Progressive Backoff (The "Security" Metric)**

This specifically mitigates **Prompt Injection** and **Brute Force** attacks.

  * **The Logic:** Attackers often use scripts to rapidly iterate through thousands of prompts to bypass safety filters.
  * **The Control:** Implement **Exponential Backoff**.
      * Request 1-10: Normal speed.
      * Request 11-20 (Rapid succession): Add 1-second artificial delay.
      * Request 20+: Add 5-second delay or temporary lockout (Cool-down period).

#### **Strategy C: Global vs. Tenant Constraints (The "Availability" Metric)**

  * **Global Limit (Infrastructure Protection):** "The GPU cluster can handle max 100 concurrent requests." (Protects the hardware).
  * **Tenant Limit (Fairness):** "Marketing Dept can use 30% of capacity; Engineering can use 50%." (Prevents one noisy neighbor from starving others).

-----

### **3. Control Definition (For Risk Matrix)**

Here is how to formalize this in your governance documentation:

| **Attribute** | **Description** |
| :--- | :--- |
| **Control Name** | **Model Resource Consumption Throttling (Rate Limiting)** |
| **Control Objective** | Prevent service degradation (DoS), control inference costs, and mitigate rapid-fire adversarial attacks (brute-force jailbreaking). |
| **Control Activity** | **1. M2M:** All API keys are assigned a strict quota (Tokens-per-minute and Requests-per-minute). Exceeding limits results in an immediate HTTP 429 error.<br>**2. H2M:** User sessions are monitored for anomalous frequency. Rapid-fire requests trigger a progressive delay (throttling) followed by a temporary session suspension.<br>**3. Cost:** Global caps are set to prevent "runaway" billing events. |
| **Implementation** | **API Gateway Level** (e.g., Kong, Apigee, AWS API Gateway). The limiter sits *in front* of the model inference service. |

-----

### **4. Recommended Technical Configuration (Sample)**

If you are guiding engineers, suggest this tiered configuration:

  * **Tier 1: The "Human" User**

      * **Rate:** 10 requests / minute.
      * **Burst:** Allow short bursts (e.g., up to 15) for rapid conversation.
      * **Response:** If exceeded, show a polite UI message: *"You're going too fast. Please wait a moment."*

  * **Tier 2: The "Background" App (M2M)**

      * **Rate:** 60 requests / minute.
      * **Token Cap:** 100,000 tokens / minute (Crucial for cost).
      * **Response:** If exceeded, return `HTTP 429 Too Many Requests` with a `Retry-After` header.

  * **Tier 3: The "Security" Circuit Breaker**

      * **Trigger:** If \>50% of requests in 1 minute result in "Safety Filter Rejection" (indicating an attack).
      * **Action:** Immediately revoke the API Key or Ban the User ID for 1 hour.

### **Next Step**

Would you like to detail the **"Circuit Breaker"** control? This is closely related but focuses on *stopping* the system entirely when error rates or toxic outputs spike (the "Emergency Stop" button).
