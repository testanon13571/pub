### **Prioritized AI/ML Model Isolation Controls**

#### **SC-AI-009: Runtime Compute Isolation (Pod & Namespace Segregation)**
* **Description:** Enforce strict process-level isolation for every model deployment. No two models should share the same container runtime or OS process namespace. Each model deployed on **Alibaba OpenTrek** must run in its own dedicated Kubernetes Pod (or equivalent container unit) with strict resource limits and blocked Inter-Process Communication (IPC).
* **Description of the Threat:** A malicious or compromised model serving container accesses the shared memory of another container on the same host to steal inference data (side-channel attack) or crashes the shared runtime, taking down other critical models.
* **Threat(s) Mitigated:**
    * **OWASP:** LLM05:2024 (Supply Chain - Compromised Runtime)
    * **MITRE ATLAS:** AML.T0043 (Craft Adversarial Data - leading to crash/escape)
* **Priority:** **Must-Have**
* **Affected Component(s):** Alibaba OpenTrek, Underlying Container Orchestrator (K8s)
* **Implementation Guidance:**
    * **Kubernetes Namespaces:** Deploy models for different Lines of Business (LOBs) in separate Namespaces.
    * **Network Policies:** Apply "Deny-All" default ingress/egress policies between Model Pods. Model A should physically not be able to ping Model B.
    * **No Shared IPC:** Ensure Docker/Container runtime `ipc_mode` is set to private (default) and not `host`.



[Image of Kubernetes Model Serving Architecture]


#### **SC-AI-010: Hardware-Level GPU Partitioning (MIG)**
* **Description:** Use NVIDIA Multi-Instance GPU (MIG) technology to partition a single physical A100/H100 GPU into multiple isolated GPU instances. This ensures that a high-load or "runaway" inference query on one model cannot starve the memory bandwidth or compute cores of a high-priority model running on the same physical card.
* **Description of the Threat:** A "Denial of Wallet" or Resource Exhaustion attack where a low-priority internal chatbot consumes 100% of the VRAM, causing the critical Fraud Detection model to fail open or time out.
* **Threat(s) Mitigated:**
    * **OWASP:** LLM04:2024 (Model Denial of Service)
    * **MITRE ATLAS:** AML.T0029 (Denial of Service)
* **Priority:** **Should-Have** (Highly recommended for cost-efficiency without sacrificing security)
* **Affected Component(s):** GPU Infrastructure, Alibaba OpenTrek
* **Implementation Guidance:**
    * Configure MIG profiles on the host driver.
    * Map specific OpenTrek model deployments to specific MIG UUIDs.
    * Ensure fault isolation: If a MIG instance crashes due to an OOM (Out of Memory) error, it must not affect other instances on the same GPU.

#### **SC-AI-011: Vector Database Tenant Isolation (Row-Level Security)**
* **Description:** Logical isolation of vector embeddings within **pgvector**. Even if multiple models share the same Postgres instance, they must never be able to query each other's knowledge base. This is the "Chinese Wall" for AI memory.
* **Description of the Threat:** A developer working on the "Marketing GenAI" model accidentally queries the vector table used by the "Private Wealth" model, retrieving sensitive client portfolio embeddings.
* **Threat(s) Mitigated:**
    * **OWASP:** LLM06:2024 (Sensitive Information Disclosure)
    * **MITRE ATLAS:** AML.T0025 (Data Leakage)
* **Priority:** **Must-Have**
* **Affected Component(s):** PostgreSQL, pgvector
* **Implementation Guidance:**
    * **Option A (Physical):** Use separate Databases or Schemas for distinct LOBs (e.g., `schema_wealth`, `schema_retail`).
    * **Option B (Logical - Recommended):** Implement PostgreSQL **Row-Level Security (RLS)**.
        * Add a `model_id` or `tenant_id` column to the `embeddings` table.
        * Create RLS policies: `CREATE POLICY isolate_tenant ON embeddings USING (tenant_id = current_setting('app.current_tenant'));`
    * Force the Application User (OpenTrek) to set the session variable `app.current_tenant` before executing any vector search.

#### **SC-AI-012: Ephemeral Cache Namespacing**
* **Description:** Ensuring that the high-speed cache (Redis) used for conversation history and semantic caching is strictly partitioned. If Model A retrieves a cached answer generated by Model B, it breaks context isolation.
* **Description of the Threat:** "Cross-Session Contamination." User A asks a question. Later, User B asks a similar question to a different model, but the semantic cache matches the vector and serves User A's (potentially sensitive) cached response.
* **Threat(s) Mitigated:**
    * **OWASP:** LLM06:2024 (Sensitive Information Disclosure)
    * **MITRE ATLAS:** AML.T0047 (LLM Session Hijacking via Cache)
* **Priority:** **Must-Have**
* **Affected Component(s):** Redis
* **Implementation Guidance:**
    * **Key Prefixing:** Enforce a strict naming convention in the application logic: `buffer:{tenant_id}:{model_id}:{user_hash}`.
    * **Logical Databases:** Assign different Redis DB indices (0-15) to different security zones, though Key Prefixing is more scalable.
    * **TTL Enforcement:** Ensure sensitive cache entries have aggressive Time-To-Live (TTL) settings so data does not linger in memory longer than necessary.

### **Summary of Isolation Strategy**

| Layer | Component | Isolation Mechanism |
| :--- | :--- | :--- |
| **Compute** | OpenTrek / K8s | **SC-AI-009:** Namespace & Pod Isolation |
| **Hardware** | GPU | **SC-AI-010:** MIG (Multi-Instance GPU) Partitioning |
| **Memory** | pgvector | **SC-AI-011:** Row-Level Security (RLS) & Schemas |
| **Cache** | Redis | **SC-AI-012:** Cryptographic Key Prefixing |







These controls provide hard isolation between models, tenants, and inference runtimes to prevent cross-model interference, cross-tenant data leakage, lateral movement, and privilege escalation in a Tier-1 global bank environment.

| Control ID | Title                                   | Description                                                                                                                   | Threat Description                                                                                                    | Threat(s) Mitigated                                      | Priority      | Affected Component(s)                                 | Implementation Guidance                                                                                                                                                                                                 |
|------------|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|---------------|-------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| SC-AI-101  | Hard Multi-Tenancy Isolation of Model Serving Runtimes | Each business unit, legal entity, or regulatory tenant operates in a completely isolated OpenTrek tenant namespace with dedicated inference pods, separate model weights, and isolated Redis/pgVector connections. No shared runtime processes across tenants. | Cross-tenant data leakage or model output mixing via shared process memory, cache, or inference runtime (high regulatory impact). | OWASP LLM06:2024 (Sensitive Information Disclosure), MITRE ATLAS AML.T0016 (Lateral Movement) | **Must-Have** | Alibaba OpenTrek, Kubernetes, Redis, pgvector         | Deploy OpenTrek using multi-tenant operator mode or separate Helm releases per tenant in dedicated Kubernetes namespaces. Enforce Pod Security Standards “restricted” + NetworkPolicy blocking inter-tenant traffic. |
| SC-AI-102  | Model-Level Container Image & Runtime Hardening | Every deployed model version runs in its own immutable container image built from a bank-approved golden base (distroless + non-root) with a unique model-specific seccomp/bpf profile and zero unnecessary binaries. | Container escape or runtime compromise enabling pivot from one model to another or to the host.                         | MITRE ATLAS AML.TA0008 (Execution), NIST AI RMF Deploy 2.2 | **Must-Have** | Alibaba OpenTrek serving runtime, Kubernetes         | Trivy + Cosign for SBOM & signature enforcement. Use gVisor or Kata Containers for highest isolation; enforce via Kyverno/Gatekeeper policies rejecting non-compliant images.                                      |
| SC-AI-103  | Cryptographic Model Weight Isolation & Key-per-Model | Model weights in MinIO are encrypted at rest with unique per-model (or per-tenant) KMS keys. Keys are never shared and are scoped to the exact model registry entry. | Exfiltration or decryption of one model’s weights granting access to another model’s embedded knowledge or IP.         | OWASP LLM02:2024 (Supply Chain), MITRE ATLAS AML.T0017 (Exfiltration) | **Must-Have** | MinIO, HashiCorp Vault / Thales CipherTrust, PostgreSQL registry | MinIO SSE-KMS with Vault transit engine; key name = model UUID. OpenTrek decrypts on-load using short-lived Vault token (Kubernetes auth). Automatic rotation + zeroisation on decommissioning. |
| SC-AI-104  | Inference-Time Memory & Context Isolation | Enforce strict per-request memory isolation: clear GPU/CPU memory and Python process state after each inference request; prohibit model-to-model in-process calling. | Residual data from one request persisting into another (memory residue side-channel).                                  | OWASP LLM06:2024 (advanced memory attacks)               | **Must-Have** | Alibaba OpenTrek serving runtime                      | Configure OpenTrek/vLLM/TGI with process-per-request or full process recycling. Disable NVIDIA MPS or clear CUDA graphs between requests.                                                                      |
| SC-AI-105  | Model Registry Namespace & Row-Level Security | Logical isolation in central PostgreSQL registry using Row-Level Security (RLS) policies so principals can only see/load models belonging to their tenant or approved domain. | Insider or compromised account loading an unauthorized model into their environment.                                  | NIST AI RMF Govern 4.2, MITRE ATLAS AML.TA0007 (Privilege Escalation) | **Must-Have** | PostgreSQL (central registry)                         | Implement RLS tied to JWT `tenant_id` claim. Example: `CREATE POLICY model_isolation ON models USING (tenant_id = current_setting('app.tenant_id'));`                                                                   |
| SC-AI-106  | Network Micro-Segmentation of Model Serving Endpoints | Zero-trust network policy: inference pods only accept mTLS traffic from OpenTrek API gateway and can only reach their own dedicated Redis/pgVector instances. | Compromised model container performing lateral movement to other models or data stores.                               | MITRE ATLAS AML.TA0016 (Lateral Movement)                | **Must-Have** | Kubernetes NetworkPolicy, Cilium/Calico               | Cilium NetworkPolicy with mTLS enforcement. Allow ingress only from gateway label; egress only to labeled Redis/pgvector per tenant.                                                                                     |
| SC-AI-107  | Model-Specific Secrets & Configuration Isolation | No shared secrets or configuration across models. Each model version has its own Kubernetes Secret/ConfigMap containing only what that exact model needs. | Secret leakage from one model granting unintended capabilities (e.g., tool credentials) to another.                    | OWASP LLM08:2024 (Excessive Agency)                      | **Should-Have** | Kubernetes Secrets, OpenTrek                          | Use Sealed Secrets or Vault CSI driver to inject per-model secrets at deployment time, referenced by model UUID.                                                                                                           |
| SC-AI-108  | Canary & Shadow Model Isolation         | New model versions deployed into physically separated canary clusters/namespaces with synthetic traffic only; production traffic never mixes with canary until full promotion. | Faulty or backdoored model version contaminating production during A/B or shadow testing.                              | NIST AI RMF Deploy 3, MITRE ATLAS AML.TA0001 (Supply Chain) | **Should-Have** | Alibaba OpenTrek, Argo Rollouts / Flagger             | Argo Rollouts with separate OpenTrek tenant pools for canary; traffic mirrored but responses discarded until manual sign-off.                                                                                     |

**Implementation Recommendation**  
Deploy all **Must-Have** controls (SC-AI-101 through SC-AI-106) before any production traffic is allowed. This baseline satisfies ECB SSRF, FED SR 11-7, MAS TRM, and PRA SS2/21 requirements for hard model and tenant isolation in supervised AI systems.
