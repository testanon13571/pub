### Step-by-Step Data Flow Description

#### Phase 1: Request & Caching (The "Speed" Layer)

  * **Step 1 (Input):** The user submits a query (e.g., *"What is the counterparty risk policy for Class A derivatives?"*) to the **Alibaba OpenTrek** API Gateway.
      * *Security Note:* This is where input sanitization (PII redaction/Injection checks) happens.
  * **Step 2 (Cache Check):** OpenTrek hashes the query and checks **Redis**.
      * **Hit:** If this exact question was asked recently, **Redis** returns the stored answer immediately (saving GPU costs).
      * **Miss:** The system proceeds to the retrieval pipeline.

#### Phase 2: Vector Retrieval (The "Search" Layer)

  * **Step 3 (Vectorization):** OpenTrek sends the raw text query to the **Embedding Model** (hosted within the OpenTrek runtime).
  * **Step 4 (Embedding):** The model converts the text into a numerical vector (e.g., a 1536-dimensional float array).
  * **Step 5 (Search):** OpenTrek sends this vector to **pgvector** (PostgreSQL). It performs an Approximate Nearest Neighbor (ANN) search (e.g., using HNSW or IVFFlat indexes) to find the most similar document chunks.
  * **Step 6 (Retrieval):** **pgvector** returns the "Top-K" chunks (e.g., the 5 most relevant paragraphs) along with their metadata (page number, document source).
      * *Note on Elasticsearch:* While your stack includes Elasticsearch, a *classic vector RAG* relies primarily on `pgvector` here. Elasticsearch would typically be used only if you needed keyword-specific filtering (e.g., "Find vectors ONLY in documents dated 2024").

#### Phase 3: Generation (The "Intelligence" Layer)

  * **Step 8 (Prompt Assembly):** OpenTrek acts as the "Chef." It combines three ingredients into a single text block:
    1.  **System Prompt:** *"You are a helpful banking assistant..."* (The immutable safety rules).
    2.  **Context:** The text chunks retrieved from **pgvector**.
    3.  **User Query:** The original question.
  * **Step 9 (Inference):** This massive block of text is sent to the **LLM (e.g., Qwen)**. The LLM reads the retrieved context and generates an answer rooted in that data.
  * **Step 10 (Response):** The answer is streamed back to the user via OpenTrek.
      * *Post-Processing:* The answer is simultaneously saved to **Redis** to speed up future requests.

### Key Component Roles Summary

| Component | Role in RAG Flow |
| :--- | :--- |
| **Alibaba OpenTrek** | **The Brain.** Orchestrates the flow, calls models, manages the prompt construction, and enforces security. |
| **Redis** | **The Accelerator.** Stores "Question -\> Answer" pairs to reduce latency. |
| **pgvector** | **The Index.** Stores the mathematical representation of your bank's knowledge. Performs the actual "search." |
| **Dell Isilon** | **The Library.** Stores the original "Gold Copy" PDF/Docx files. If `pgvector` only stores vectors, OpenTrek fetches the actual readable text from here (or a text cache) to show the user. |
